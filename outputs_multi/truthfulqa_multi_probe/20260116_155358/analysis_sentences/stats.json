{
  "correctness_by_category": {
    "cohen_d": 0.01049568597227335,
    "mean_adversarial": 8.597315788269043,
    "mean_non_adversarial": 8.56862735748291,
    "n_adversarial": 149,
    "n_non_adversarial": 153,
    "p_value": 0.927456316857764,
    "sem_adversarial": 0.23012695001318229,
    "sem_non_adversarial": 0.21485203144191828
  },
  "matching": {
    "items_unmatched": 0,
    "sentences_total": 302,
    "sentences_unmatched": 1
  },
  "probe_metrics": {
    "fabrication_vs_truthfulness": {
      "max": {
        "intercept": 0.33300128800357187,
        "n": 302,
        "p_value": 0.6197209272257709,
        "r": -0.02866872027516365,
        "slope": -0.003446477944410028
      },
      "mean": {
        "intercept": -0.02078131893840859,
        "n": 302,
        "p_value": 0.23656200892482776,
        "r": -0.06831356883049011,
        "slope": -0.006366934094139365
      },
      "prop_pos": {
        "intercept": 0.4252751915756959,
        "n": 302,
        "p_value": 0.8525780311779986,
        "r": -0.01073753647506237,
        "slope": -0.0012146505864730494
      }
    },
    "lying_vs_truthfulness": {
      "max": {
        "intercept": 0.15959405873772026,
        "n": 302,
        "p_value": 8.082870213567263e-05,
        "r": -0.22485363483428955,
        "slope": -0.03621374028750605
      },
      "mean": {
        "intercept": -0.4108279382257096,
        "n": 302,
        "p_value": 8.828575316946234e-05,
        "r": -0.223674476146698,
        "slope": -0.02788125184773653
      },
      "prop_pos": {
        "intercept": 0.1612354798850374,
        "n": 302,
        "p_value": 0.0002583151459532792,
        "r": -0.20880046486854553,
        "slope": -0.011110834657387455
      }
    }
  }
}